# -*- coding: utf-8 -*-
"""Copy of b2_FakeNewDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UzDP9R4GWeujwax-dIMDhoCzqSYmbjtK

## Setup Environment
"""

# Install specific libraries
! pip install transformers
'''transformers is a Python library developed by Hugging Face.

Purpose: It provides state-of-the-art pre-trained models for:

Natural Language Processing (NLP) → text classification, translation, summarization, sentiment analysis, question answering, etc.

Computer Vision (CV) → image classification, object detection.

Multimodal tasks (text + image).

Key Feature: Instead of training deep learning models from scratch, you can load pre-trained models (like BERT, GPT, T5, ViT) and fine-tune them easily.'''
! pip install pycaret
'''pycaret is a low-code machine learning library in Python.

Purpose: It automates ML workflows → from preprocessing, training, and hyperparameter tuning to evaluation and deployment.

Key Feature: With just a few lines of code, you can compare multiple models, pick the best one, and even deploy it.'''

import numpy as np
import pandas as pd
import transformers
from transformers import AutoModel, BertTokenizerFast
'''AutoModel

A generic class that automatically picks the right model architecture based on the name of the pre-trained model you load.

For example:

from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-uncased")


Here, AutoModel recognizes "bert-base-uncased" and loads a BERT model.

Advantage: You don’t need to manually decide if it’s BERT, RoBERTa, DistilBERT, etc. Hugging Face handles that for you.

Usually used for getting embeddings/hidden states rather than direct predictions.
BertTokenizerFast

Tokenizers convert text → tokens → numerical IDs so the model can understand text.

BertTokenizerFast is a fast implementation (written in Rust) of the original BERT tokenizer.'''
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import torch #Brings in the PyTorch library (used for deep learning & tensor operations, similar to NumPy but with GPU support).
import torch.nn as nn #Imports the neural network module.

#nn contains layers, loss functions, and model building blocks (e.g., nn.Linear, nn.ReLU, nn.CrossEntropyLoss, etc.).
# specify GPU
device = torch.device("cuda")
'''device = torch.device("cuda")

Sets the computing device.

"cuda" means: if you have a GPU with CUDA enabled (like NVIDIA GPUs), PyTorch will use it.

If no GPU is available, you should usually fallback to CPU:

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
'''

# Mount Google Drive - applicable, if working on Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Set Working Directory - if working on Google Drive
# %cd /content/drive/MyDrive/1_LiveProjects/Project11_FakeNewsDetection

# # Set Working Directory - if working on Local Machine
# import os
# os.chdir('/Users//replace_me')

"""## Load Dataset"""

# Load Dataset
try:
    true_data = pd.read_csv('a1_True.csv')
    fake_data = pd.read_csv('a2_Fake.csv')

    # Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'
    true_data['Target'] = ['True']*len(true_data)
    fake_data['Target'] = ['Fake']*len(fake_data)

    # Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'
    data = pd.concat([true_data, fake_data]).sample(frac=1).reset_index().drop(columns=['index'])
    '''pd.concat([true_data, fake_data])

Concatenates (stacks) the two DataFrames:

true_data → DataFrame of true samples

fake_data → DataFrame of fake samples

Now you have one big DataFrame with both classes.

.sample(frac=1)

Shuffles all rows.

frac=1 means 100% of the data is returned, but in random order.

.reset_index()

Resets the index after shuffling.

Old indices (from original dataframes) become a new column called index.

.drop(columns=['index'])

Removes that extra index column since you don’t need it anymore.'''

    # See how the data looks like
    print(data.shape)
    display(data.head())

except FileNotFoundError:
    print("Make sure 'a1_True.csv' and 'a2_Fake.csv' are in the correct directory.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

  '''except FileNotFoundError:

Catches the specific error when a file is missing.

Example: if you try to pd.read_csv("a1_True.csv") but the file isn’t in your folder, this block runs.

print("Make sure 'a1_True.csv' and 'a2_Fake.csv' are in the correct directory.")

Gives the user a helpful message about what went wrong.

except Exception as e:

A catch-all for any other unexpected error (syntax errors, permission errors, etc.).

as e lets you print the actual error message.

print(f"An unexpected error occurred: {e}")

Uses an f-string to show the error details.'''

# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=1)
data['label'] = pd.get_dummies(data.Target)['Fake']

data.head()

# Checking if our data is well balanced
label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]
plt.pie(label_size,explode=[0.1,0.1],colors=['firebrick','navy'],startangle=90,shadow=True,labels=['Fake','True'],autopct='%1.1f%%')
'''plt.pie(
    label_size,                        # Data values (counts or percentages)
    explode=[0.1, 0.1],                # Pops out slices slightly
    colors=['firebrick', 'navy'],      # Slice colors
    startangle=90,                     # Rotate so chart starts from top
    shadow=True,                       # Add a shadow effect
    labels=['Fake', 'True'],           # Labels for slices
    autopct='%1.1f%%'                  # Show percentage values on chart
)'''

"""## Train-test-split"""

# Train-Validation-Test set split into 70:15:15 ratio
# Train-Temp split
train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'],
                                                                    random_state=2018,
                                                                    test_size=0.3,
                                                                    stratify=data['Target'])
# Validation-Test split
val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,
                                                                random_state=2018,
                                                                test_size=0.5,
                                                                stratify=temp_labels)

"""## BERT Fine-tuning

### Load pretrained BERT Model
"""

# Load BERT model and tokenizer via HuggingFace Transformers
bert = AutoModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

"""### Prepare Input Data"""

# Plot histogram of the number of words in train data 'title'
seq_len = [len(title.split()) for title in train_text]

pd.Series(seq_len).hist(bins = 40,color='firebrick')
plt.xlabel('Number of Words')
plt.ylabel('Number of texts')

# BERT Tokeizer Functionality
sample_data = ["Build fake news model.",
               "Using bert."]                                         # sample data
tokenized_sample_data = tokenizer.batch_encode_plus(sample_data,
                                                    padding=True)     # encode text
print(tokenized_sample_data)

# Ref: https://huggingface.co/docs/transformers/preprocessing

# Majority of titles above have word length under 15. So, we set max title length as 15
MAX_LENGHT = 15
# Tokenize and encode sequences in the train set
tokens_train = tokenizer.batch_encode_plus(
    train_text.tolist(),
    max_length = MAX_LENGHT,
    padding=True,
    truncation=True
)
# tokenize and encode sequences in the validation set
tokens_val = tokenizer.batch_encode_plus(
    val_text.tolist(),
    max_length = MAX_LENGHT,
    padding=True,
    truncation=True
)
# tokenize and encode sequences in the test set
tokens_test = tokenizer.batch_encode_plus(
    test_text.tolist(),
    max_length = MAX_LENGHT,
    padding=True,
    truncation=True
)

# Convert lists to tensors
train_seq = torch.tensor(tokens_train['input_ids'])
train_mask = torch.tensor(tokens_train['attention_mask'])
train_y = torch.tensor(train_labels.tolist()).long()

val_seq = torch.tensor(tokens_val['input_ids'])
val_mask = torch.tensor(tokens_val['attention_mask'])
val_y = torch.tensor(val_labels.tolist()).long()

test_seq = torch.tensor(tokens_test['input_ids'])
test_mask = torch.tensor(tokens_test['attention_mask'])
test_y = torch.tensor(test_labels.tolist()).long()

# Data Loader structure definition
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32                                               #define a batch size

train_data = TensorDataset(train_seq, train_mask, train_y)    # wrap tensors
train_sampler = RandomSampler(train_data)                     # sampler for sampling the data during training
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
                                                              # dataLoader for train set
val_data = TensorDataset(val_seq, val_mask, val_y)            # wrap tensors
val_sampler = SequentialSampler(val_data)                     # sampler for sampling the data during training
val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)
                                                              # dataLoader for validation set

"""### Freeze Layers"""

# Freezing the parameters and defining trainable BERT structure
for param in bert.parameters():
    param.requires_grad = False    # false here means gradient need not be computed

"""### Define Model Architecture"""

class BERT_Arch(nn.Module):
    def __init__(self, bert):
      super(BERT_Arch, self).__init__()
      self.bert = bert
      self.dropout = nn.Dropout(0.1)            # dropout layer
      self.relu =  nn.ReLU()                    # relu activation function
      self.fc1 = nn.Linear(768,512)             # dense layer 1
      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)
      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function
    def forward(self, sent_id, mask):           # define the forward pass
      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']
                                                # pass the inputs to the model
      x = self.fc1(cls_hs)
      x = self.relu(x)
      x = self.dropout(x)
      x = self.fc2(x)                           # output layer
      x = self.softmax(x)                       # apply softmax activation
      return x

model = BERT_Arch(bert)
# Defining the hyperparameters (optimizer, weights of the classes and the epochs)
# Define the optimizer
import torch.optim as optim
optimizer = optim.Adam(model.parameters(),
                       lr = 1e-5)          # learning rate
# Define the loss function
cross_entropy  = nn.NLLLoss()
# Number of training epochs
epochs = 2

"""### Define Train & Evaluate Function"""

# Defining training and evaluation functions
def train():
  model.train()
  total_loss, total_accuracy = 0, 0

  for step,batch in enumerate(train_dataloader):                # iterate over batches
    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))
    batch = [r for r in batch]                                  # push the batch to gpu
    sent_id, mask, labels = batch
    model.zero_grad()                                           # clear previously calculated gradients
    preds = model(sent_id, mask)                                # get model predictions for current batch
    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values
    total_loss = total_loss + loss.item()                       # add on to the total loss
    loss.backward()                                             # backward pass to calculate the gradients
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem
    optimizer.step()                                            # update parameters
    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU

  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch
                                                                # reshape predictions in form of (# samples, # classes)
  return avg_loss                                 # returns the loss and predictions

def evaluate():
  print("\nEvaluating...")
  model.eval()                                    # Deactivate dropout layers
  total_loss, total_accuracy = 0, 0
  for step,batch in enumerate(val_dataloader):    # Iterate over batches
    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.
                                                  # Calculate elapsed time in minutes.
                                                  # Elapsed = format_time(time.time() - t0)
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))
                                                  # Report progress
    batch = [t for t in batch]                    # Push the batch to GPU
    sent_id, mask, labels = batch
    with torch.no_grad():                         # Deactivate autograd
      preds = model(sent_id, mask)                # Model predictions
      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values
      total_loss = total_loss + loss.item()
      preds = preds.detach().cpu().numpy()
  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch
  return avg_loss

"""### Model training"""

# Train and predict
best_valid_loss = float('inf')
train_losses=[]                   # empty lists to store training and validation loss of each epoch
valid_losses=[]

for epoch in range(epochs):
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))
    train_loss = train()                       # train model
    valid_loss = evaluate()                    # evaluate model
    if valid_loss < best_valid_loss:              # save the best model
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'c2_new_model_weights.pt')
    train_losses.append(train_loss)               # append training and validation loss
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')

"""### Model performance"""

# load weights of best model
path = 'c2_new_model_weights.pt'
model.load_state_dict(torch.load(path))

with torch.no_grad():
  preds = model(test_seq, test_mask)
  preds = preds.detach().cpu().numpy()

preds = np.argmax(preds, axis = 1)
print(classification_report(test_y, preds))

"""## Fake News Predictions"""

# # load weights of best model
# path = 'c1_fakenews_weights.pt'
# model.load_state_dict(torch.load(path))

# testing on unseen data
unseen_news_text = ["Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing",     # Fake
                    "WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy",               # Fake
                    "U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources",           # True
                    "Trump administration issues new rules on U.S. visa waivers"                          # True
                    ]

# tokenize and encode sequences in the test set
MAX_LENGHT = 15
tokens_unseen = tokenizer.batch_encode_plus(
    unseen_news_text,
    max_length = MAX_LENGHT,
    padding=True,
    truncation=True
)

unseen_seq = torch.tensor(tokens_unseen['input_ids'])
unseen_mask = torch.tensor(tokens_unseen['attention_mask'])

with torch.no_grad():
  preds = model(unseen_seq, unseen_mask)
  preds = preds.detach().cpu().numpy()

preds = np.argmax(preds, axis = 1)
preds

! pip uninstall numpy pandas -y
! pip install pandas